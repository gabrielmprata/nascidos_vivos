# -*- coding: utf-8 -*-
"""DataSus_NascidoVivos_PreProc_V02.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uhnUL13C96OZ3AvjdQiZBFTqu9AmwlTJ

<img loading="lazy" src="https://cdn.jsdelivr.net/gh/devicons/devicon@latest/icons/python/python-original.svg" width="40" height="40"/> <img src="https://cdn.jsdelivr.net/gh/devicons/devicon@latest/icons/pandas/pandas-original-wordmark.svg" width="40" height="40"/>   <img loading="lazy" src="https://cdn.jsdelivr.net/gh/devicons/devicon@latest/icons/plotly/plotly-original-wordmark.svg" width="40" height="40"/>

---
**Pré Processamento de dados**
>
**Dev**: Gabriel Prata
>
**Data**: 29/04/2024
>
**Última modificação**: 29/05/2024
>
**Contexto**: *Dados abertos de Nascidos Vivos*
>
---

![Badge em Desenvolvimento](http://img.shields.io/static/v1?label=STATUS&message=CONCLUIDO&color=GREEN&style=for-the-badge)

#**<font color=#4c60d6 size="6"> Import libraries**
"""

# Importação de pacotes
import pandas as pd
import numpy as np
import missingno as ms # para tratamento de missings
import datetime
import re # expressão regulares

#bibliotecas para visualização de dados
import seaborn as sns
import plotly.express as px
import matplotlib.pyplot as plt

# Configuração para não exibir os warnings
import warnings
warnings.filterwarnings("ignore")

"""#**<font color=#4c60d6 size="6"> 1. Objetivo**

Esse Notebook, foca na etapa de pré-processamento de dados.
>
Esta é a etapa mais demorada e trabalhosa do projeto de ciência de dados, e estima-se que consuma pelo menos 70% do tempo total do projeto.
>
Ao final desse notebook, entregaremos um arquivo mais coeso para as análises dos dados de Nascidos Vivos no Brasil.

#**<font color=#4c60d6 size="6"> 2. Coleta de dados**

###**<font color=#4c60d6> 2.1 Nascidos Vivos**

As informações dos **Nascidos Vivos**, estão no sítio de dados abertos do OpenDataSus, no link abaixo:
>
![OpenDatasusLogobranca.png](data:image/png;base64,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)
>
https://opendatasus.saude.gov.br/dataset/sistema-de-informacao-sobre-nascidos-vivos-sinasc
>
Ou uma opção com um link direto, sem a necessidade de fazer download do arquivo.
>
https://opendatasus.saude.gov.br/dataset/sistema-de-informacao-sobre-nascidos-vivos-sinasc/resource/72a97e9d-e525-4f26-9ecd-b5eff6cd1796
"""

# Informa a URL de importação do dataset
url = "https://s3.sa-east-1.amazonaws.com/ckan.saude.gov.br/SINASC/DNOPEN23.csv"

# Lê o arquivo (60s)
dataset = pd.read_csv(url, delimiter=';')

"""###**<font color=#4c60d6> 2.2 Datasets auxiliares**

Carga de datasets auxiliares, para enriquecer as informações do dataset principal.

####**<font color=#4c60d6> 2.2.1 CNES**

Dataset auxiliar, com as informações dos estabelecimentos de saúde.
"""

# Informa a URL de importação do dataset
url2 = "https://s3.sa-east-1.amazonaws.com/ckan.saude.gov.br/CNES/cnes_estabelecimentos.zip"

# Lê o arquivo 60s
df_cnes = pd.read_csv(url2, compression='zip', encoding = "Latin 1", delimiter=';')

"""####**<font color=#4c60d6> 2.2.2 Municípios IBGE API**

Iremos utilizar a API do IBGE, para coletar todos os códigos dos municípios brasileiros.
>
Essas informações, irão enriquecer o Dataset com o nome do município, estado e a unidade da federação.
"""

pip install ibge

# Importando os pacotes da biblioteca de localidades
from ibge.localidades import *

# Função Estados() retorna um objeto do tipo ibge.localidades.Estados
#dados = Estados()
dados = Municipios()

type(dados)

regiao = dados.json()

#pandas json_normalize ler a string JSON aninhada e devolver um DataFrame.
estados_br = pd.json_normalize(regiao)

estados_br.info()

# Criar um data frame com as colunas que iremos utilizar
tb_municipios = estados_br[['id','nome', 'microrregiao.mesorregiao.UF.sigla','microrregiao.mesorregiao.UF.regiao.nome'
                          ]]

# Renomear campos

tb_municipios = tb_municipios.rename(columns={'nome': 'municipio',
                                              'microrregiao.mesorregiao.UF.sigla': 'uf',
                                              'microrregiao.mesorregiao.UF.regiao.nome': 'regiao'
                                             })

tb_municipios.head(3)

"""O código do Municipío na Fato, considera apenas os 6 primeiros dígitos.
>
Sendo assim, vamos manipular o atributo ID.
"""

# Transfomar em string
tb_municipios['id'] = tb_municipios['id'].map(str)

# Pegar os 6 primeiros digitos
tb_municipios['id'] = (tb_municipios['id'].str[:6]).astype(int)

"""####**<font color=#4c60d6> 2.2.3 CBO**

A Classificação Brasileira de Ocupações - CBO, instituída por portaria ministerial nº. 397, de 9 de outubro de 2002, tem por finalidade a identificação das ocupações no mercado de trabalho, para fins classificatórios.
>
Com essa informação, iremos saber a ocupação da mãe.
"""

tb_cbo = pd.read_csv('CBO.csv', encoding = "utf-8", sep=',')

tb_cbo.head()

"""#**<font color=#4c60d6 size="6"> 3. Análise de Dados Inicial**

###**<font color=#4c60d6> 3.1. Estatísticas Descritivas**

Compreende a organização, o resumo e, descrever os dados, que podem ser expressos em tabelas e gráficos.
>
Veremos a seguir alguns comandos para exibir algumas estatísticas descritivas.
>
---

><font color=#4c60d6>**Dataset Nascidos Vivos**
"""

#	Quantidade de atributos e instâncias (linhas/colunas)
dataset.shape

"""<font color=#4c60d6> Data frame com 62 atributos(features) e 2.5MM tuplas.

---
"""

# Exibir os 5 primeiros registros
dataset.head(5)

"""

---

"""

# Mostra diversas informações do Dataframe em um único comando, e exibir o uso de memória
dataset.info(memory_usage="deep")

"""<font color=#4c60d6> Data frame utilizando 1.4 GB de memória.


---
"""

# Quantidade de valores únicos
dataset.nunique()

"""

---

"""

# Quantidade de NaN/Missing/Nulls no dataframe
dataset.isnull().sum()

"""><font color=#4c60d6>**Dataset CNES**"""

#	Quantidade de atributos e instâncias (linhas/colunas)
df_cnes.shape

"""

---

"""

# Exibir os 3 primeiros registros
df_cnes.head(3)

"""

---

"""

# Mostra diversas informações do Dataframe em um único comando, e exibir o uso de memória
df_cnes.info(memory_usage="deep")

"""

---

"""

# Quantidade de valores únicos
df_cnes.nunique()

"""

---

"""

# Quantidade de NaN/Missing/Nulls no dataframe
df_cnes.isnull().sum()

"""###**<font color=#4c60d6> 3.2. Distribuição dos atributos**

>Nessa etapa, iremos verificar a distribuição dos principais atributos. Para ver se existe a necessidade de tomar alguma ação de transformações na etapa de preparação de dados.


---
"""

dataset[['IDADEMAE', 'IDADEPAI', 'SEXO','PESO','QTDGESTANT','SEMAGESTAC','QTDFILVIVO','QTDFILMORT','QTDPARTNOR','QTDPARTCES','CONSPRENAT','MESPRENAT']].describe().round(2)

"""O atributo idade da mãe(idademae), é importante para as análises, e identificamos um valor máximo de 99 anos.
>
Acredito que quando não sabem a idade da mãe, entram com 99 no sistema para que possa efetuar o registro.
>
O mesmo pode acontecer para quantidade de gestação anteriores, quantidade de filhos vivos, quantidade de filhos mortos, quantidade de parto normal, quantidade de parto cesária.
>
O atributo SEXO, segundo o dicionario de dados, só pode ser preenchido com 1 - Masculino, 2 - Feminino e 3 - ignorado. Pela distribuição encontramos os 3 valores, sem nenhum valor fora do esperado.
"""

dataset.SEXO.value_counts() #quantidade

"""No atributo peso, encontramos valores de 100 gramas e de 7000 gramas(7kg).
>
A quantidade de registros com esses valores, são pequenas, em relação ao total.
>
Por isso a importancia de ter um especialista no assun to para definir os críterios.
>
Como é um estudo acadêmico, vamos seguir com ainformação como está.
"""

verifica_peso = dataset.groupby(['PESO'])['contador'].count().reset_index()
verifica_peso

dataset.PESO.value_counts() #quantidade

"""#**<font color=#4c60d6 size="6"> 4. Pré-Processamento de dados**

Após coletar e analisar os dados na etapa anterior, agora é o momento
de limpar, transformar e apresentar melhor os dados.
>
Assim poderemos obter, na próxima etapa, os melhores resultados possíveis nos algoritmos de
Machine Learning, ou simplesmente apresentar dados mais confiáveis para os clientes em soluções de
business intelligence.


---

###**<font color=#4c60d6> 4.1. Limpeza**

De forma resumida, a limpeza consiste na verificação da consistência das informações, correção de possíveis erros de preenchimento ou eliminação de valores desconhecidos, redundantes ou não pertencentes ao domínio.

####**<font color=#4c60d6> 4.1.1 Redundâncias**

Vamos eliminar as colunas que não iremos utilizar em nossas analises.
>
A ideia é ter um dataframe mais leve, e com pouco espaço em disco.

><font color=#4c60d6>**Dataset Nascidos Vivos**
"""

dataset.drop([
              'contador','ORIGEM','ESCMAE','QTDFILVIVO','QTDFILMORT','CODMUNRES','PARTO',
              'HORANASC','DTCADASTRO','CODANOMAL','NUMEROLOTE','VERSAOSIST','DTRECEBIM','DIFDATA','OPORT_DN','DTRECORIGA',
              'NATURALMAE','CODMUNNATU','CODUFNATU','ESCMAE2010','SERIESCMAE','DTNASCMAE','QTDPARTNOR','QTDPARTCES','IDADEPAI',
              'DTULTMENST','TPMETESTIM','CONSPRENAT','TPAPRESENT','STTRABPART','STCESPARTO','TPFUNCRESP','TPDOCRESP','DTDECLARAC',
              'STDNEPIDEM','STDNNOVA','CODPAISRES','PARIDADE'
             ], axis=1, inplace= True)

dataset.info()

"""Reduzimos de 62 atributos, para 24.

><font color=#4c60d6>**Dataset CNES**
"""

df_cnes.drop([
              'CO_UNIDADE',
              'CO_UF',
              'CO_IBGE',
              'NU_CNPJ_MANTENEDORA',
              'CO_NATUREZA_ORGANIZACAO',
              'CO_NIVEL_HIERARQUIA',
              'DS_NIVEL_HIERARQUIA',
              'CO_ESFERA_ADMINISTRATIVA',
              'CO_ATIVIDADE',
              'CO_CEP',
              'NO_LOGRADOURO',
              'NU_ENDERECO',
              'NO_BAIRRO',
              'NU_TELEFONE',
              'NU_LATITUDE',
              'NU_LONGITUDE',
              'CO_TURNO_ATENDIMENTO',
              'NU_CNPJ',
              'NO_EMAIL',
              'ST_CENTRO_CIRURGICO',
              'ST_CENTRO_OBSTETRICO',
              'ST_CENTRO_NEONATAL',
              'ST_ATEND_HOSPITALAR',
              'ST_SERVICO_APOIO',
              'ST_ATEND_AMBULATORIAL',
              'CO_MOTIVO_DESAB',
              'DS_NATUREZA_ORGANIZACAO',
              'DS_ESFERA_ADMINISTRATIVA'
             ], axis=1, inplace= True)

df_cnes.info()

"""####**<font color=#4c60d6> 4.1.2 Padronização de dados**

Dentro da programação, possuímos alguns padrões de escrita para nomes de variáveis, funções, classes e assim por diante.
>
Esses padrões de escrita são chamados de estilos de case.
>
Existem diversos tipos de case, nesse projeto iremos utilizar:
>
**Snake Case (snake_case)**: Nesse estilo, todas as letras são minúsculas e as palavras são separadas por um underscore(_).
"""

# Criar uma função para aplicar o snake_case
def snake_case(string):
    string = re.sub(" +", " ", string)   # substitui múltiplos espaços por um espaço
    string = re.sub(" ", "_", string)    # substitui espaço por _
    return string.lower() # transforma em minuscula

dataset.columns = [snake_case(column) for column in dataset.columns]
dataset.columns

# dataset CNES
df_cnes.columns = [snake_case(column) for column in df_cnes.columns]
df_cnes.columns

"""####**<font color=#4c60d6> 4.1.3 Tratamento de Missings**

><font color=#4c60d6>**Dataset Nascidos Vivos**

Como o DataFrame tem muitos atributos, e muitos deles possuem valores nulos, vou utlizar um método para mostrar apenas os valores nulos e o percentual.
>
1) Converter a função isnull() em um pd.series e depois transformar em DF, assim conseguimos filtrar quais atributos são nulos.
>
"""

#1)
# cria um pd.series
dfnull = dataset.isnull().sum()

# Converte series em dataframe
dfnull = (dfnull.to_frame(name="QTD"))

tot = len(dataset) #Total de registros no dataset

#Criano o atributo perc, para saber o percentual de registros nulo do atributo
dfnull["perc"] = ((dfnull['QTD']/tot)*100).round(2)

#Mostrar apenas os atributos com valores nulos, ordenando para o com mais linhas nulas
dfnull.query('QTD > 0').sort_values(by='perc', ascending=False)

"""---

2) Usar a librarie **MISSINGNO**, que retorna um avisão gráfica bem interessante.
"""

#A Barra mostra a quantidade de registros preenchidos
#Sort ordena do mais preenchido para o menos sort="ascending"
#Sem o sort="ascending", aparece pela ordem dos campos

ms.bar(dataset, color="dodgerblue",  figsize=(20,15), fontsize=12)

"""O atributo ***codocupmae***, código da ocupação da mãe, é o que tem mais valores nulos, seguido de ***racacormae***, raça/cor da mãe.
>
Não iremos excluir os registros para não prejudicar as analises quantitativas de nascimentos.
>
Vamos substituir por um valor e depois tratar no item 4.2.1, atribuido a informação: "Ignorado"
"""

# substituir
dataset['racacormae'] = dataset['racacormae'].fillna(9)
dataset['mesprenat'] = dataset['mesprenat'].fillna(99)
dataset['qtdgestant'] = dataset['qtdgestant'].fillna(99)
dataset['racacor'] = dataset['racacor'].fillna(9)
dataset['escmaeagr1'] = dataset['escmaeagr1'].fillna(9)

dataset['apgar1'] = dataset['apgar1'].fillna(99)
dataset['apgar5'] = dataset['apgar5'].fillna(99)
dataset['semagestac'] = dataset['semagestac'].fillna(99)
dataset['gestacao'] = dataset['gestacao'].fillna(9)
dataset['tpnascassi'] = dataset['tpnascassi'].fillna(9)
dataset['idanomal'] = dataset['idanomal'].fillna(9)
dataset['estcivmae'] = dataset['estcivmae'].fillna(9)
dataset['consultas'] = dataset['consultas'].fillna(9)
dataset['gravidez'] = dataset['gravidez'].fillna(9)
dataset['peso'] = dataset['peso'].fillna(0)
dataset['idademae'] = dataset['idademae'].fillna(0)

"""####**<font color=#4c60d6> 4.1.4 Outliers**

><font color=#4c60d6>**Dataset Nascidos Vivos**

>**<font color=#4c60d6> Idade da mãe**

Como o dataset têm muitos registros, vamos agrupar o box pela raça/cor da mãe.
"""

import plotly.express as px

fig = px.box(dataset, x='racacormae', y="idademae")
fig.show()

dataset.idademae.describe().round(2)

""">**<font color=#4c60d6> Sexo**"""

dataset.sexo.value_counts() #quantidade

import plotly.express as px

fig = px.box(dataset, x='racacormae', y="sexo")
fig.show()

""">**<font color=#4c60d6> Peso**"""

import plotly.express as px

fig = px.box(dataset, y="peso")
fig.show()

"""A título acadêmico, iremos substituir todos os nascidos com peso menor que 500 gramas, por um valor aliatório, entre o Q1 e Q3.
>
Q1 = 2875
>
Q3 = 3496
"""

from random import randint #gera números inteiros
dataset.loc[dataset['peso'] <= 500, 'peso'] = randint(2875,3496)

"""###**<font color=#4c60d6> 4.2 Criação de recursos**

Também conhecida como ***feature engineering***, a criação de recursos consiste em criar, a partir dos atributos originais, um conjunto de atributos que capture informações importantes.

####**<font color=#4c60d6> 4.2.1 Construção de recuros**

O dataset traz muitos campos com a informação em código numérico.
>
Por exemplo, o sexo do recém nascido está codificado da seguinte maneira:
>
Sexo: 1- M – Masculino; 2- F – Feminino; 0- I – ignorado
>
Sendo assim, precisamos tratar os campos que estejam com essa codificação.

><font color=#4c60d6>**Nascidos Vivos**
"""

# Criar um dicionário com a codificação dos campos
diclocnasc = {
    1: "Hospital",
    2: "Outrso estabelecimentos de saúde",
    3: "Domicílio",
    4: "Outros",
    5: "Aldeia Indígina",
    9: "Ignorado"
}

dicestcvmae = {
    1: "Solteira",
    2: "Casada",
    3: "Viúva",
    4: "Divorciada",
    5: "União estável",
    9: "Ignorado"
}

dicescmae = {
    1 : "Nenhuma",
    2 : "1 a 3 anos",
    3 : "4 a 7 anos",
    4 : "8 a 11 anos",
    5 : "12 e mais",
    9 : "Ignorado"
}

dicgestacao = {
    1: "Menos de 22 semanas",
    2: "22 a 27 semanas",
    3: "28 a 31 semanas",
    4: "32 a 36 semanas",
    5: "37 a 41 semanas",
    6: "42 semanas e mais",
    9: "Ignorado"
}

dicgravidez = {
    1: "Única",
    2: "Dupla",
    3: "Tripla ou mais",
    9: "Ignorado"
}

dictparto = {
              1: "Vaginal",
              2: "Cesário",
              9: "Ignorado"

}

dicconsultas = {
        1: "Nenhuma",
        2: "de 1 a 3",
        3: "de 4 a 6",
        4: "7 e mais",
        9: "Ignorado"
}

dicsexo = {
    1 : "Masculino",
    2 : "Feminino",
    0 : "Ignorado",
    9 : "Ignorado"
}

dicracacor = {
    1 : "Branca",
    2 : "Preta",
    3 : "Amarela",
    4 : "Parda",
    5 : "Indígena",
    0 : "Ignorado",
    9 : "Ignorado"
}

dicracacormae = {
    1 : "Branca",
    2 : "Preta",
    3 : "Amarela",
    4 : "Parda",
    5 : "Indígena",
    0 : "Ignorado",
    9 : "Ignorado"
}

dicidanomal = {
    1: "Sim",
    2: "Não",
    9 : "Ignorado"
}

dicmesprenat = {
    1: "1º Mês",
    2: "2º Mês",
    3: "3º Mês",
    4: "4º Mês",
    5: "5º Mês",
    6: "6º Mês",
    7: "7º Mês",
    8: "8º Mês",
    9: "9º Mês",
    99: "Ignorado"
}

dictpapresent = {
1: "Cefálico",
2: "Pélvica ou podálica",
3: "Transversa",
9: "Ignorado"
}

dicsttrabpart = {
    1: "Sim",
    2: "Não",
    3: "Não se aplica",
    9: "Ignorado"
}

dictpnascassi = {
    1: "Médico",
    2: "Enfermagem ou Obstetriz",
    3: "Parteira",
    4: "Outros",
    9: "Ignorado"
}

dicescmaeagr1 = {
    0 : "Sem Escolaridade",
    1 : "Fundamental I Incompleto",
    2 : "Fundamental I Completo",
    3 : "Fundamental II Incompleto",
    4 : "Fundamental II Completo",
    5 : "Ensino Médio Incompleto",
    6 : "Ensino Médio Completo",
    7 : "Superior Incompleto",
    8 : "Superior Completo",
    9 : "Ignorado",
    10 : "Fundamental I Incompleto ou Inespecífico",
    11 : "Fundamental II Incompleto ou Inespecífico",
    12 : "Ensino Médio Incompleto ou Inespecífico"
}

dicapgar1 = {
    0 : "Apgar < 7",
    1 : "Apgar < 7",
    2 : "Apgar < 7",
    3 : "Apgar < 7",
    4 : "Apgar < 7",
    5 : "Apgar < 7",
    6 : "Apgar < 7",
    7 : "Apgar >= 7",
    8 : "Apgar >= 7",
    9 : "Apgar >= 7",
   10 : "Apgar >= 7",
   99 : "Apgar ignorado"
}

dicapgar5 = {
    0 : "Apgar < 7",
    1 : "Apgar < 7",
    2 : "Apgar < 7",
    3 : "Apgar < 7",
    4 : "Apgar < 7",
    5 : "Apgar < 7",
    6 : "Apgar < 7",
    7 : "Apgar >= 7",
    8 : "Apgar >= 7",
    9 : "Apgar >= 7",
   10 : "Apgar >= 7",
   99 : "Apgar ignorado"
}

# Fazer o replace nos atributos conforme o dicionario
dataset = dataset.replace({
    'locnasc' : diclocnasc,
    'estcivmae' : dicestcvmae,
    'escmae' : dicescmae,
    'gestacao' : dicgestacao,
    'gravidez' : dicgravidez,
    'parto' : dictparto,
    'consultas' : dicconsultas,
    'sexo' : dicsexo,
    'racacor' : dicracacor,
    'racacormae' : dicracacormae,
    'idanomal' : dicidanomal,
    'mesprenat' : dicmesprenat,
    'tpapresent' : dictpapresent,
    'sttrabpart' : dicsttrabpart,
    'tpnascassi' : dictpnascassi,
    'escmaeagr1' : dicescmaeagr1,
    'apgar1' : dicapgar1,
    'apgar5' : dicapgar5

})

"""><font color=#4c60d6>**CNES - Cadastro de estabelecimentos de saúde**"""

# alterando campo com codificação
dictp_gestao = {
    "M" : "Municipal",
    "E" : "Estadual",
    "D" : "Dupla",
    "S" : "Sem Gestão"
}

# Replace
df_cnes = df_cnes.replace({'tp_gestao' : dictp_gestao})

# apagando os campos que não serão mais necessários
df_cnes.drop([
              'tp_unidade',
              'co_natureza_jur',
              'ds_turno_atendimento',
              'no_razao_social'
             ], axis=1, inplace= True)

"""> <font color=#4c60d6>**Campo data**"""

# Transfomar em string
dataset['dtnasc'] = dataset['dtnasc'].map(str)

# Pegar 6 ultimos caracteres que seria MMAAA
dataset['mes_ano'] = dataset['dtnasc'].str[-6:]

# Pegar 4 ultimos que seria o ano
dataset['ano'] = dataset['mes_ano'].str[-4:]

# Pegar os 2 primeiros que seria o mes
dataset['mes'] = dataset['mes_ano'].str[:2]

# Concatenar para ficar no padrão de data AAAA-MM
dataset['ano_mes'] = dataset['ano']+'-'+dataset['mes']

"""> <font color=#4c60d6>**Faixa de idade da mãe**"""

# Cria o atributo com as idades
dataset['faixa_etaria'] = dataset['idademae']

# Range das faixas
classes = [-1, 2, 14, 19, 24, 29, 34, 39 , 500]

# Nome das faixas
labels = ['Ignorado', '00 a 14', '15 a 19', '20 a 24', '25 a 29',
          '30 a 34','35 a 39','40 a +']

# Aplica faixas
classes = pd.cut(x=dataset.faixa_etaria, bins=classes, labels=labels)
dataset['faixa_etaria'] = classes

#Verificar o tipo do atributo com a faixa que criamos
dataset['faixa_etaria'].dtypes

# Como o tipo é category, vamos transformar em STR
dataset['faixa_etaria'] = dataset['faixa_etaria'].astype(str)

#valida
dataset.faixa_etaria.value_counts()

"""> <font color=#4c60d6>**Faixa peso do RN**"""

# Cria o atributo com os pesos
dataset['faixa_peso'] = dataset['peso']

# Range das faixas
classes = [-1, 1, 499, 999, 1499, 2499, 2999, 3999, 9000]

# Nome das faixas
labels = ['Ignorado', '< 500', '500 a 999', '1000 a 1499', '1500 a 2499',
          '2500 a 2999','3000 a 3999','4000 a +']

# Aplica faixas
classes = pd.cut(x=dataset.faixa_peso, bins=classes, labels=labels)
dataset['faixa_peso'] = classes

#Verificar o tipo do atributo com a faixa que criamos
dataset.dtypes['faixa_peso']

# Como o tipo é category, vamos transformar em STR
dataset['faixa_peso'] = dataset['faixa_peso'].astype(str)

dataset.faixa_peso.value_counts().sort_values()

"""---

####**<font color=#4c60d6> 4.2.2 Enriquecimento**

Existem outros atributos que iremos relacionar com outro dataset.
>
Seria como um modelo relacional, porém nesse caso, iremos adicionar no dataset principal(Fato), os atributos das dimensões.

><font color=#4c60d6>**CNES - Cadastro de estabelecimentos de saúde**
"""

# Verificando se existe duplicidade na dimensão
df_cnes.duplicated(subset='co_cnes', keep='first').sum()

# Left Join entre a Fato e a dimensão
dataset = pd.merge(dataset, df_cnes, left_on=['codestab'], right_on=['co_cnes'], how='left')

"""><font color=#4c60d6>**Municípios**

Nessa etapa iremos fazer o merge da Fato com a dimensão de municípios.
"""

dataset = pd.merge(dataset, tb_municipios, left_on=['codmunnasc'], right_on=['id'], how='left')

"""><font color=#4c60d6>**CBO**

Nessa etapa iremos fazer o merge da Fato com a dimensão de ocupações.
"""

dataset = pd.merge(dataset, tb_cbo, left_on=['codocupmae'], right_on=['CODIGO'], how='left')

"""###**<font color=#4c60d6> 4.3 Redução da dimensionalidade**

Os datasets podem ter muitas características, e muitos algoritmos de Machine Learning funcionam melhor se a dimensionalidade for menor.

####**<font color=#4c60d6> 4.3.1 Agregação**

Também pode ser considerada uma técnica de redução de dimensionalidade, pois reduz o número de colunas e linhas do dataset.
>
O nosso dataset, é aberto por Municipio, e nesse momento não iremos analisar nessa granularidade.
>
Vamos criar um dataset agregado com a visão por UF, agrupandos os atributos que não se repetem.
>
Assim, será possivel fazer analises mais rápidas, e ter um dataset menor para usar no Github e Streamlit.
"""

# Vamos criar o atributo qtd(quantidade) para facilitar as analises
dataset['qtd'] = 1

# Criar um dataset com os campos finais necessarios para utilizar no Streamlit
tb_final = dataset[['locnasc','idademae','estcivmae','gestacao','gravidez','consultas','sexo','apgar1','apgar5',
'racacor','peso','idanomal','racacormae','qtdgestant','semagestac','mesprenat','tpnascassi',
'escmaeagr1','tprobson','ano_mes','faixa_etaria','faixa_peso','tp_gestao','uf','regiao',
'qtd']]

"""A função de GROUP BY do Pandas, só agrupa campos que não sejam nulos.
>
Sendo assim, para agregar iremos identificar quais atributos que iremos utilizar, e que estão nulos.
"""

#1)
# cria um pd.series
dfnull = tb_final.isnull().sum()

# Converte series em dataframe
dfnull = (dfnull.to_frame(name="QTD"))

tot = len(tb_final) #Total de registros no dataset

#Criano o atributo perc, para saber o percentual de registros nulo do atributo
dfnull["perc"] = ((dfnull['QTD']/tot)*100).round(2)

#Mostrar apenas os atributos com valores nulos, ordenando para o com mais linhas nulas
dfnull.query('QTD > 0').sort_values(by='perc', ascending=False)

tb_final['tp_gestao'] = tb_final['tp_gestao'].fillna('NI')
tb_final['uf'] = tb_final['uf'].fillna('NI')
tb_final['regiao'] = tb_final['regiao'].fillna('NI')

#tb_final['racacormae'] = tb_final['racacormae'].fillna('NI')
#tb_final['mesprenat'] = tb_final['mesprenat'].fillna('NI')
#tb_final['qtdgestant'] = tb_final['qtdgestant'].fillna(99)
#tb_final['racacor'] = tb_final['racacor'].fillna('NI')
#tb_final['tp_gestao'] = tb_final['tp_gestao'].fillna('NI')
#tb_final['escmaeagr1'] = tb_final['escmaeagr1'].fillna('NI')
#tb_final['apgar1'] = tb_final['apgar1'].fillna('NI')
#tb_final['apgar5'] = tb_final['apgar5'].fillna('NI')
#tb_final['semagestac'] = tb_final['semagestac'].fillna(99)
#tb_final['gestacao'] = tb_final['gestacao'].fillna('NI')
#tb_final['tpnascassi'] = tb_final['tpnascassi'].fillna('NI')
#tb_final['idanomal'] = tb_final['idanomal'].fillna('NI')
#tb_final['estcivmae'] = tb_final['estcivmae'].fillna('NI')
#tb_final['consultas'] = tb_final['consultas'].fillna('NI')
#tb_final['gravidez'] = tb_final['gravidez'].fillna('NI')
#tb_final['peso'] = tb_final['peso'].fillna(99)
#tb_final['uf'] = tb_final['uf'].fillna('NI')
#tb_final['idademae'] = tb_final['idademae'].fillna('NI')
#tb_final['regiao'] = tb_final['regiao'].fillna('NI')

# Agrupando o dataset
df_sinasc = tb_final.groupby(['locnasc','idademae','estcivmae','gestacao','gravidez','consultas','sexo','apgar1','apgar5',
'racacor','peso','idanomal','racacormae','qtdgestant','semagestac','mesprenat','tpnascassi',
'escmaeagr1','tprobson','ano_mes','faixa_etaria','faixa_peso','tp_gestao','uf','regiao'])['qtd'].sum().reset_index()

# Exportar para csv
df_sinasc.to_csv('df_sinasc.csv', index=False)

# compactar arquivo
import shutil
shutil.make_archive('df_sinasc', 'zip', './', 'df_sinasc.csv')

df_total_uf = df_sinasc.groupby(["regiao"])['qtd'].sum().reset_index()
df_total_uf

"""Como o GitHub só comporta arquivos de até 25 Mg, vou criar mais 2 arquivos para criar visualizações no Streamlit."""

# Analise por municipios
df_municipios = dataset.groupby(["ano_mes","regiao","uf","municipio"])['qtd'].sum().reset_index()

df_cbo = ((dataset.groupby(["OCUPACAO"])['qtd'].sum().reset_index()).sort_values(by='qtd', ascending=False)).head(10)

"""#**<font color=#4c60d6 size="6"> 5. Export**

Agora iremos exportar o dataset em CSV, para usarmos no Streamlit
"""

# Export to CSV
df_sinasc.to_csv('df_sinasc.csv', index=False)

# Compactar arquivo
import shutil

shutil.make_archive('df_sinasc', 'zip', './', 'df_sinasc.csv')

# Export to CSV
df_municipios.to_csv('df_municipios.csv', index=False)

# Compactar arquivo
import shutil

shutil.make_archive('df_municipios', 'zip', './', 'df_municipios.csv')

# Export to CSV
df_cbo.to_csv('df_cbo.csv', index=False)

# Compactar arquivo
import shutil

shutil.make_archive('df_cbo', 'zip', './', 'df_cbo.csv')